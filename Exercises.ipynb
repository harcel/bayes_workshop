{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import emcee\n",
    "from astroML.plotting import plot_mcmc\n",
    "\n",
    "\n",
    "# These defaults are the ones I like, but feel free to adapt them\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises on Bayesian statistics and inference\n",
    "These are exercises belonging to the workshop material on Bayesian statistics and inference \"Bayesian Statistics: the what, the why, and the how\". I leave it totally up to you whether you do any if this or not. In the Solutions notebook you will find one way to tackle the problems posed here. Most, if not all, problems can be solved in a plethora of ways. Feel free to experiment around, it will only teach you more in the end.\n",
    "\n",
    "## 1) Testing positively to a serious disease\n",
    "\n",
    "Here are the equations for the disease testing exercise. Plug in some numbers and play around if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_positive_if_ill = .99\n",
    "P_positive_if_notill = .01\n",
    "P_ill = 1.e-3\n",
    "\n",
    "\n",
    "def P_ill_if_positive(P_positive_if_ill=.99, P_positive_if_notill=.01, P_ill=1.e-3):\n",
    "    \"\"\"Function that simply plugs the numbers into Bayes' theorem\"\"\"\n",
    "    P_notill = 1 - P_ill\n",
    "    return P_positive_if_ill * P_ill / (P_positive_if_ill * P_ill + P_positive_if_notill * P_notill )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09016393442622951\n"
     ]
    }
   ],
   "source": [
    "# Feel free to play around with these numbers!\n",
    "print(P_ill_if_positive(P_positive_if_ill=.99, P_positive_if_notill=.01, P_ill=1.e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions you could ask yourself are:\n",
    "- What number would the pharmaceutical industry like to see optimized compared to the example? \n",
    "- Most people guess a much higher probability than 0.09 when given the question. What aspect are they forgetting about, and how/where does this appear in Bayes' theorem? \n",
    "- What happens when you forget the terms about false positives? \n",
    "- Investigate the dependence of the posterior probability graphically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Updating one's knowledge of a system\n",
    "\n",
    "Given the numbers in the original example, as stated above, how many positive tests do you need to get before your certainty about being ill rises above 99%? \n",
    "It's probably easiest to write a while-loop (or a for-loop with a break statement) in which Bayes' theorem is used to update the knowledge with the new data every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) A world of Gaussians\n",
    "\n",
    "For the situation where we try to determine the mean flux of that star, while it is itself fluctuating in brightness, please recreate the sampler, try to do so from scratch, but if that is too much to ask, then find some inspiration in the notebook.\n",
    "\n",
    "If that works well: try the following:\n",
    "- Play with the number of walkers. Does it make much of a difference if you use only one (hint: emcee will tell you!)?\n",
    "- Try to not use a flat prior, but a very wide Gaussian for the mean, still flat for the standard deviation. Try this with the gaussian centered at the \"correct\" value, and with Gaussians that do include the correct value at a low but non-zero number of standard deviations from the correct values and one that is far off. Discuss outcomes! It may help to make the intrinsic distribution of fluxes wider (and/or the sample really small).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, here's the code that generates the data set:\n",
    "np.random.seed(42)  \n",
    "N = 100  \n",
    "mu_true, sigma_true = 1000, 10  # True flux at time of measurement is distributed following a gaussian.\n",
    "\n",
    "F_true = stats.norm(mu_true, sigma_true).rvs(N)  # Onbekende werkelijke aantallen, nu met scatter\n",
    "F = stats.poisson(F_true).rvs()  # Waargenomen aantallen, met errors\n",
    "e = np.sqrt(F)  # root-N error\n",
    "\n",
    "# For the visual, a graph of that:\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(F, np.arange(N), xerr=e, fmt='ok', ecolor='gray', alpha=0.5)\n",
    "ax.vlines([F_true], 0, N, linewidth=5, alpha=0.1)\n",
    "ax.set_xlabel(\"F\");ax.set_ylabel(\"Measurement number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The likelihood, prior and resulting posterior:\n",
    "\n",
    "# Setting up the emcee run (think of dimensions, walkers, starting guesses)\n",
    "\n",
    "# Use the tab completion help from the docstring to see how to call the run!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4) The linear fit with outliers, with outliers.\n",
    "\n",
    "In the notebook we saw how to fit a slope and intercept to points with errors in the vertical direction. We reproduce the data set here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([ 0,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n",
    "              40, 41, 42, 43, 54, 56, 67, 69, 72, 88])\n",
    "y = np.array([33, 68, 34, 34, 37, 71, 37, 44, 48, 49,\n",
    "              53, 49, 50, 48, 56, 60, 61, 63, 44, 71])\n",
    "e = np.array([ 3.6, 3.9, 2.6, 3.4, 3.8, 3.8, 2.2, 2.1, 2.3, 3.8,\n",
    "               2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])\n",
    "\n",
    "plt.errorbar(x, y, e, fmt='.k', ecolor='gray')\n",
    "plt.xlabel('X');plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a reminder, we will give the definition of the likelihood, which is a bit complicated, and the definition of the posterior, we leave the definition of the prior to you! Do make sure to check if you understand the likelihood function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, e, sigma_B):\n",
    "    dy = y - theta[0] - theta[1] * x\n",
    "    g = np.clip(theta[2:], 0, 1)  # g<0 or g>1 leads to NaNs in logarithm\n",
    "    logL1 = np.log(g) - 0.5 * np.log(2 * np.pi * e ** 2) - 0.5 * (dy / e) ** 2\n",
    "    logL2 = np.log(1 - g) - 0.5 * np.log(2 * np.pi * sigma_B ** 2) - 0.5 * (dy / sigma_B) ** 2\n",
    "    return np.sum(np.logaddexp(logL1, logL2))\n",
    "\n",
    "def log_posterior(theta, x, y, e, sigma_B):\n",
    "    return log_prior(theta) + log_likelihood(theta, x, y, e, sigma_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the posterior shows what will need to go into the prior and what comes. What is it? \n",
    "Also define the prior as a function. Think of the allowed parameter ranges, don't cheat and look the solutions notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the MCMC, in the same set-up as in the notebook, to make sure we get the weird points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2 + len(x)  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "nburn = 10000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 15000  # number of MCMC steps to take\n",
    "\n",
    "\n",
    "# set theta near the maximum likelihood, with \n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "def squared_loss(theta, x=x, y=y, e=e):\n",
    "    dy = y - theta[0] - theta[1] * x\n",
    "    return np.sum(0.5 * (dy / e) ** 2)\n",
    "\n",
    "theta1 = optimize.fmin(squared_loss, [0, 0], disp=False)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "starting_guesses = np.zeros((nwalkers, ndim))\n",
    "starting_guesses[:, :2] = np.random.normal(theta1, 1, (nwalkers, 2))\n",
    "starting_guesses[:, 2:] = np.random.normal(0.5, 0.1, (nwalkers, ndim - 2))\n",
    "\n",
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x, y, e, 50])\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    "\n",
    "sample = sampler.chain  # shape = (nwalkers, nsteps, ndim)\n",
    "sample = sampler.chain[:, nburn:, :].reshape(-1, ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plot_mcmc(sample[:,:2].T, fig=fig, labels=[r'Intercept', r'slope'], colors='k')\n",
    "ax[0].plot(sample[:, 0], sample[:, 1], '.k', alpha=0.1, ms=4);\n",
    "ax[0].plot([mu_true], [sigma_true], 'o', color='red', ms=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the attributes that come with teh sampler object. Is there any way in which you can assess the evolution of the sampled posterior probability density? What does it look like? You probably want to investigate the evolution of that pdf along all the walkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to run this with a different radom number seed, most likely you find no deviant points. Somehow, this particular walker took very long to get the equilibrium region where it should be sampling. Always check if your choice of burn-in period seems appropriate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Truncated exponentials the numeric way\n",
    "\n",
    "We have seen the Jaynes' truncated exponential in the instruction notebook. We have found an analytical way to come up with the 95% confidence interval. Because we like numerical sampling, let's have a try and construct an MCMC sampling of this posterior and get the 95% confidence interval numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are valus for $\\theta$ in the sample that are higher than the common sense upper limit (likely). Investigate if your burn-in period is taken long enough. Can there be other explanations?\n",
    "\n",
    "Clean up your sample after burn in by discarding points that have an infintely low probabiilty (the attributes of the sample that is the result of the MCMC run should have some hints) and estimate the 95% credible region again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have some time left...\n",
    "\n",
    "Think of your own work. Where would it all fit in? Do you maximize likelihood functions every once in a while? And if so: do you actually have prior knowledge you would like to take into account? Are there nuisance parameters that you ignore and just pick some value for because that made the statistics easier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workshop]",
   "language": "python",
   "name": "conda-env-workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
